# Neural Machine Translation
I built a Neural Machine Translation (NMT) model to convert human-readable dates (e.g., "25th of June, 2009") into machine-readable format ("2009-06-25") using an attention-based sequence-to-sequence model. I first implemented one_step_attention(), calculating context vectors with shared weights across time steps. Then, I created the full model (modelf()), using a Bi-LSTM and attention mechanism to generate outputs at each time step. Finally, I compiled the model with Adam optimizer (learning_rate=0.005) and used categorical_crossentropy as the loss function and accuracy as the metric.
