# Regularization
Implementing techniques to prevent overfitting in deep learning models! The exercises included adding L2 regularization to the cost function and backpropagation, penalizing large weights to encourage simpler models. I also applied dropout during forward propagation to randomly deactivate neurons and integrated this into the backpropagation process to ensure consistency.
