# Regularization
Implementing regularization techniques to prevent overfitting in deep learning models! The exercises involved adding L2 regularization to both the cost function and the backpropagation process, ensuring that large weights are penalized to encourage simpler, more generalizable models. I also implemented dropout during forward propagation to randomly deactivate neurons, further preventing overfitting by making the model less reliant on any single feature. Finally, I integrated dropout into the backpropagation process to maintain consistency with the forward pass.
