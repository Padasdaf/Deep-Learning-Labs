# Regularization
Applying L2 regularization and dropout to a deep learning model aimed at recommending optimal positions for football players! The exercises involved implementing L2 regularization to penalize large weights and prevent overfitting, by adjusting the cost function and the gradients during backpropagation. Additionally, I integrated dropout into the forward and backward propagation processes to randomly shut down neurons during training, further reducing the risk of overfitting. These techniques were applied to a three-layer neural network to enhance its ability to generalize to new data, making the model more robust and effective in providing recommendations.
