# Initialization
Exploring various methods for initializing the weights of a neural network, which is crucial for effective training! The exercises involved implementing zeros initialization, where all weights are set to zero, random initialization with large random values, and He initialization, which uses a scaled normal distribution tailored for ReLU activations. Each method was tested to observe its impact on the learning process, with a focus on how initialization affects the convergence speed of gradient descent and the overall training and generalization performance of the model.
