# Initialization
Exploring different methods for initializing neural network weights, which is crucial for effective training! I implemented zeros initialization by setting all weights to zero, random initialization using large random values, and He initialization using a scaled normal distribution for ReLU activations. Each method was tested to observe its impact on the learning process, particularly in terms of gradient descent convergence speed and the model's training and generalization performance.
